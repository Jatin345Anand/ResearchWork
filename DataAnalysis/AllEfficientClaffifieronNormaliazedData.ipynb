{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "crops = pd.read_csv('crop_production.csv')\n",
    "crops.dropna(inplace=True)\n",
    "import numpy as np\n",
    "X1= crops['Area'].values\n",
    "X2= crops['Production'].values\n",
    "nx1= np.array(X1)\n",
    "nx2= np.array(X2)\n",
    "Y = crops['Season'].values\n",
    "def Reduct_N_DigitValues(n,L):\n",
    "    Ans=[]\n",
    "    for i in range(len(L)):\n",
    "        if(len(str(int(round(L[i]))))>(n-1)):\n",
    "            Ans.append(int(str(int(round(L[i])))[:n]))\n",
    "    return np.array(Ans)\n",
    "nx2lists=[]\n",
    "for i in range(1,6):\n",
    "    nx2lists.append(Reduct_N_DigitValues(i,X2))\n",
    "nx1lists=[]\n",
    "for i in range(1,6):\n",
    "    nx1lists.append(Reduct_N_DigitValues(i,X1))\n",
    "def MinimizeLengthofLists(L1,L2,L3):\n",
    "    Ans=[]\n",
    "    L11= L1.tolist()\n",
    "    L22= L2.tolist()\n",
    "    Min = min(len(L11),len(L22))\n",
    "#     print(\"min = \",Min)\n",
    "    if(Min==len(L11)):\n",
    "#         print('L11')\n",
    "        for i in range(Min):\n",
    "            Ans.append(L22[i])\n",
    "        L22.clear()\n",
    "        L22=Ans\n",
    "    else:\n",
    "#         print('L22')\n",
    "        for i in range(Min):\n",
    "            Ans.append(L11[i])\n",
    "        L11.clear()\n",
    "        L11=Ans\n",
    "#     print(len(L11),len(L22),len(Ans),len(L3))\n",
    "    Y1=[]\n",
    "    for i in range(Min):\n",
    "        Y1.append(L3[i])\n",
    "#     print(len(Y1))\n",
    "    return np.array(L11),np.array(L22),np.array(Y1)\n",
    "L=[]\n",
    "for i in range(5):\n",
    "    L1=[]\n",
    "    Ans= MinimizeLengthofLists(nx1lists[i],nx2lists[i],Y)\n",
    "    L1.append(Ans[0])\n",
    "    L1.append(Ans[1])\n",
    "    L1.append(Ans[2])\n",
    "    L.append(L1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "NX = []\n",
    "for i in L:\n",
    "    NX.append(np.vstack((i[0],i[1])).T)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANSNormalized = normalize(NX[0], norm='l2', axis=1, copy=True, return_norm=False)\n",
    "# L[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.4472136 , 0.89442719],\n",
       "       [0.89442719, 0.4472136 ],\n",
       "       [0.31622777, 0.9486833 ],\n",
       "       ...,\n",
       "       [0.9486833 , 0.31622777],\n",
       "       [0.37139068, 0.92847669],\n",
       "       [0.12403473, 0.99227788]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ANSNormalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import  StandardScaler\n",
    "Scaler = StandardScaler()\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainingTestData=[]\n",
    "for i in range(5):\n",
    "    NX[i]= Scaler.fit_transform(NX[i])\n",
    "    NX[i]= Scaler.transform(NX[i])\n",
    "    NX[i] = normalize(NX[i], norm='l2', axis=1, copy=True, return_norm=False)\n",
    "    TrainingTestData.append(train_test_split(NX[i],L[i][2],test_size=.4, random_state=42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "RFclf = RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1)\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knnclf = KNeighborsClassifier(3)\n",
    "from sklearn import tree\n",
    "DTclf = tree.DecisionTreeClassifier(max_depth=5)\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "GPCclf = GaussianProcessClassifier(1.0 * RBF(1.0))\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "MLPclf = MLPClassifier(alpha=1)\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "Gussclf = GaussianNB()\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "Adbclf = AdaBoostClassifier()\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "QDAclf = QuadraticDiscriminantAnalysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "PredictList=[]\n",
    "ScoreList=[]\n",
    "def GetScorePredictValue(L,clf):\n",
    "    clf.fit(L[0],L[2])\n",
    "    Predict = clf.predict(L[1])\n",
    "    Score = clf.score(L[1],L[3])\n",
    "#     accuracy = accuracy_score(L[3],Predict)\n",
    "    return Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "RFPA=[]\n",
    "for i in TrainingTestData:\n",
    "    RFPA.append(GetScorePredictValue(i,RFclf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3962865542317809,\n",
       " 0.3913083194409555,\n",
       " 0.3840646715982963,\n",
       " 0.37062804676037675,\n",
       " 0.338466985307134]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RFPA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "AdaPA=[]\n",
    "for i in TrainingTestData:\n",
    "    AdaPA.append(GetScorePredictValue(i,Adbclf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3962865542317809,\n",
       " 0.3912740642627144,\n",
       " 0.3840366509751177,\n",
       " 0.369492167163614,\n",
       " 0.33651973800672685]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AdaPA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "DTPA=[]\n",
    "for i in TrainingTestData:\n",
    "    DTPA.append(GetScorePredictValue(i,DTclf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3962865542317809,\n",
       " 0.3912283906917262,\n",
       " 0.38400863035193905,\n",
       " 0.3709120166595674,\n",
       " 0.33722782793414763]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DTPA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "QdaPA=[]\n",
    "for i in TrainingTestData:\n",
    "    QdaPA.append(GetScorePredictValue(i,QDAclf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.38736396926092115,\n",
       " 0.39131973783370255,\n",
       " 0.3842327953373683,\n",
       " 0.3706753750769085,\n",
       " 0.3356936330914026]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QdaPA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "GussPA=[]\n",
    "for i in TrainingTestData:\n",
    "    GussPA.append(GetScorePredictValue(i,Gussclf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.38736396926092115,\n",
       " 0.39131973783370255,\n",
       " 0.3842327953373683,\n",
       " 0.3706753750769085,\n",
       " 0.33710981294624415]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GussPA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "KnnPA=[]\n",
    "for i in TrainingTestData:\n",
    "    KnnPA.append(GetScorePredictValue(i,knnclf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.30892774253442673,\n",
       " 0.3225695951037932,\n",
       " 0.31759975341851604,\n",
       " 0.3127218514837427,\n",
       " 0.29521449224051455]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KnnPA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate Algo\n",
    "#Cross Validation\n",
    "#Accuracy Metric\n",
    "#Random Forest\n",
    "#bagging_predict\n",
    "#Subsample\n",
    "#BuildTree\n",
    "#Get Split\n",
    "#Split\n",
    "#to_terminal\n",
    "#gini index\n",
    "#predict\n",
    "\n",
    "\n",
    "#seeding the generated number makes our results reproducible (good for debugging)\n",
    "from random import seed\n",
    "from random import randrange\n",
    "from csv import reader\n",
    "from math import sqrt\n",
    "\n",
    "# Load a CSV file\n",
    "def load_csv(filename):\n",
    "\tdataset = list()\n",
    "\twith open(filename, 'r') as file:\n",
    "\t\tcsv_reader = reader(file)\n",
    "\t\tfor row in csv_reader:\n",
    "\t\t\tif not row:\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tdataset.append(row)\n",
    "\treturn dataset\n",
    " \n",
    "# Convert string column to float\n",
    "def str_column_to_float(dataset, column):\n",
    "    #iterate throw all the rows in our data matrix\n",
    "\tfor row in dataset:\n",
    "        #for the given column index, convert all values in that column to floats\n",
    "\t\trow[column] = float(row[column].strip())\n",
    " \n",
    "# Convert string column to integer\n",
    "def str_column_to_int(dataset, column):\n",
    "    #store a given column \n",
    "    class_values = [row[column] for row in dataset]\n",
    "    #create an unordered collection with no duplicates, only unique valeus\n",
    "    unique = set(class_values)\n",
    "    #init a lookup table\n",
    "    lookup = dict()\n",
    "    #for each element in the column\n",
    "    for i, value in enumerate(unique):\n",
    "        #add it to our lookup table\n",
    "        lookup[value] = i\n",
    "    #the lookup table stores pointers to the strings\n",
    "    for row in dataset:\n",
    "        row[column] = lookup[row[column]]\n",
    "    #return the lookup table\n",
    "    return lookup\n",
    "\n",
    "def cross_validation_split(dataset, n_folds):\n",
    "\tdataset_split = list()\n",
    "\tdataset_copy = list(dataset)\n",
    "\tfold_size = int(len(dataset) / n_folds)\n",
    "\tfor i in range(n_folds):\n",
    "\t\tfold = list()\n",
    "\t\twhile len(fold) < fold_size:\n",
    "\t\t\tindex = randrange(len(dataset_copy))\n",
    "\t\t\tfold.append(dataset_copy.pop(index))\n",
    "\t\tdataset_split.append(fold)\n",
    "\treturn dataset_split\n",
    "\n",
    "# Split a dataset based on an attribute and an attribute value\n",
    "def test_split(index, value, dataset):\n",
    "    #init 2 empty lists for storing split dataubsets\n",
    "\tleft, right = list(), list()\n",
    "    #for every row\n",
    "\tfor row in dataset:\n",
    "        #if the value at that row is less than the given value\n",
    "\t\tif row[index] < value:\n",
    "            #add it to list 1\n",
    "\t\t\tleft.append(row)\n",
    "\t\telse:\n",
    "            #else add it list 2 \n",
    "\t\t\tright.append(row)\n",
    "    #return both lists\n",
    "\treturn left, right\n",
    " \n",
    "# Calculate accuracy percentage\n",
    "def accuracy_metric(actual, predicted):\n",
    "\tcorrect = 0\n",
    "\tfor i in range(len(actual)):\n",
    "\t\tif actual[i] == predicted[i]:\n",
    "\t\t\tcorrect += 1\n",
    "\treturn correct / float(len(actual)) * 100.0\n",
    " \n",
    "# Evaluate an algorithm using a cross validation split\n",
    "def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n",
    "\tfolds = cross_validation_split(dataset, n_folds)\n",
    "\tscores = list()\n",
    "\tfor fold in folds:\n",
    "\t\ttrain_set = list(folds)\n",
    "\t\ttrain_set.remove(fold)\n",
    "\t\ttrain_set = sum(train_set, [])\n",
    "\t\ttest_set = list()\n",
    "        #add each row in a given subsample to the test set\n",
    "\t\tfor row in fold:\n",
    "\t\t\trow_copy = list(row)\n",
    "\t\t\ttest_set.append(row_copy)\n",
    "\t\t\trow_copy[-1] = None\n",
    "\t\tpredicted = algorithm(train_set, test_set, *args)\n",
    "\t\tactual = [row[-1] for row in fold]\n",
    "\t\taccuracy = accuracy_metric(actual, predicted)\n",
    "\t\tscores.append(accuracy)\n",
    "\treturn scores\n",
    " \n",
    " \n",
    "# Calculate the Gini index for a split dataset\n",
    "## this is the name of the cost function used to evaluate splits in the dataset.\n",
    "# this is a measure of how often a randomly chosen element from the set \n",
    "#would be incorrectly labeled if it was randomly labeled according to the distribution\n",
    "#of labels in the subset. Can be computed by summing the probability\n",
    "#of an item with label i being chosen times the probability \n",
    "#of a mistake in categorizing that item. \n",
    "#It reaches its minimum (zero) when all cases in the node \n",
    "#fall into a single target category.\n",
    "#A split in the dataset involves one input attribute and one value for that attribute. \n",
    "#It can be used to divide training patterns into two groups of rows.\n",
    "#A Gini score gives an idea of how good a split is by how mixed the classes \n",
    "#are in the two groups created by the split. A perfect separation results in \n",
    "#a Gini score of 0, whereas the worst case split that results in 50/50 classes \n",
    "#in each group results in a Gini score of 1.0 (for a 2 class problem).\n",
    "#We first need to calculate the proportion of classes in each group.\n",
    "def gini_index(groups, class_values):\n",
    "\tgini = 0.0\n",
    "    #for each class\n",
    "\tfor class_value in class_values:\n",
    "        #a random subset of that class\n",
    "\t\tfor group in groups:\n",
    "\t\t\tsize = len(group)\n",
    "\t\t\tif size == 0:\n",
    "\t\t\t\tcontinue\n",
    "            #average of all class values\n",
    "\t\t\tproportion = [row[-1] for row in group].count(class_value) / float(size)\n",
    "            #  sum all (p * 1-p) values, this is gini index\n",
    "\t\t\tgini += (proportion * (1.0 - proportion))\n",
    "\treturn gini\n",
    " \n",
    "# Select the best split point for a dataset\n",
    "#This is an exhaustive and greedy algorithm\n",
    "def get_split(dataset, n_features):\n",
    "    ##Given a dataset, we must check every value on each attribute as a candidate split, \n",
    "    #evaluate the cost of the split and find the best possible split we could make.\n",
    "\tclass_values = list(set(row[-1] for row in dataset))\n",
    "\tb_index, b_value, b_score, b_groups = 999, 999, 999, None\n",
    "\tfeatures = list()\n",
    "\twhile len(features) < n_features:\n",
    "\t\tindex = randrange(len(dataset[0])-1)\n",
    "\t\tif index not in features:\n",
    "\t\t\tfeatures.append(index)\n",
    "\tfor index in features:\n",
    "\t\tfor row in dataset:\n",
    "            ##When selecting the best split and using it as a new node for the tree \n",
    "            #we will store the index of the chosen attribute, the value of that attribute \n",
    "            #by which to split and the two groups of data split by the chosen split point.\n",
    "            ##Each group of data is its own small dataset of just those rows assigned to the \n",
    "            #left or right group by the splitting process. You can imagine how we might split \n",
    "            #each group again, recursively as we build out our decision tree.\n",
    "\t\t\tgroups = test_split(index, row[index], dataset)\n",
    "\t\t\tgini = gini_index(groups, class_values)\n",
    "\t\t\tif gini < b_score:\n",
    "\t\t\t\tb_index, b_value, b_score, b_groups = index, row[index], gini, groups\n",
    "    ##Once the best split is found, we can use it as a node in our decision tree.\n",
    "    ##We will use a dictionary to represent a node in the decision tree as \n",
    "    #we can store data by name. \n",
    "\treturn {'index':b_index, 'value':b_value, 'groups':b_groups}\n",
    " \n",
    "# Create a terminal node value\n",
    "\n",
    "def to_terminal(group):\n",
    "    #select a class value for a group of rows. \n",
    "\toutcomes = [row[-1] for row in group]\n",
    "    #returns the most common output value in a list of rows.\n",
    "\treturn max(set(outcomes), key=outcomes.count)\n",
    " \n",
    "#Create child splits for a node or make terminal\n",
    "#Building a decision tree involves calling the above developed get_split() function over \n",
    "#and over again on the groups created for each node.\n",
    "#New nodes added to an existing node are called child nodes. \n",
    "#A node may have zero children (a terminal node), one child (one side makes a prediction directly) \n",
    "#or two child nodes. We will refer to the child nodes as left and right in the dictionary representation \n",
    "#of a given node.\n",
    "#Once a node is created, we can create child nodes recursively on each group of data from \n",
    "#the split by calling the same function again.\n",
    "def split(node, max_depth, min_size, n_features, depth):\n",
    "    #Firstly, the two groups of data split by the node are extracted for use and \n",
    "    #deleted from the node. As we work on these groups the node no longer requires access to these data.\n",
    "\tleft, right = node['groups']\n",
    "\tdel(node['groups'])\n",
    "    \n",
    "    #Next, we check if either left or right group of rows is empty and if so we create \n",
    "    #a terminal node using what records we do have.\n",
    "\t# check for a no split\n",
    "\tif not left or not right:\n",
    "\t\tnode['left'] = node['right'] = to_terminal(left + right)\n",
    "\t\treturn\n",
    "    #We then check if we have reached our maximum depth and if so we create a terminal node.\n",
    "\t# check for max depth\n",
    "\tif depth >= max_depth:\n",
    "\t\tnode['left'], node['right'] = to_terminal(left), to_terminal(right)\n",
    "\t\treturn\n",
    "    #We then process the left child, creating a terminal node if the group of rows is too small, \n",
    "    #otherwise creating and adding the left node in a depth first fashion until the bottom of \n",
    "    #the tree is reached on this branch.\n",
    "\t# process left child\n",
    "\tif len(left) <= min_size:\n",
    "\t\tnode['left'] = to_terminal(left)\n",
    "\telse:\n",
    "\t\tnode['left'] = get_split(left, n_features)\n",
    "\t\tsplit(node['left'], max_depth, min_size, n_features, depth+1)\n",
    "\t# process right child\n",
    "    #The right side is then processed in the same manner, \n",
    "    #as we rise back up the constructed tree to the root.\n",
    "\tif len(right) <= min_size:\n",
    "\t\tnode['right'] = to_terminal(right)\n",
    "\telse:\n",
    "\t\tnode['right'] = get_split(right, n_features)\n",
    "\t\tsplit(node['right'], max_depth, min_size, n_features, depth+1)\n",
    " \n",
    "#Build a decision tree\n",
    "def build_tree(train, max_depth, min_size, n_features):\n",
    "    #Building the tree involves creating the root node and \n",
    "\troot = get_split(train, n_features)\n",
    "    #calling the split() function that then calls itself recursively to build out the whole tree.\n",
    "\tsplit(root, max_depth, min_size, n_features, 1)\n",
    "\treturn root\n",
    " \n",
    "# Make a prediction with a decision tree\n",
    "def predict(node, row):\n",
    "    #Making predictions with a decision tree involves navigating the \n",
    "    #tree with the specifically provided row of data.\n",
    "    #Again, we can implement this using a recursive function, where the same prediction routine is \n",
    "    #called again with the left or the right child nodes, depending on how the split affects the provided data.\n",
    "    #We must check if a child node is either a terminal value to be returned as the prediction\n",
    "    #, or if it is a dictionary node containing another level of the tree to be considered.\n",
    "\tif row[node['index']] < node['value']:\n",
    "\t\tif isinstance(node['left'], dict):\n",
    "\t\t\treturn predict(node['left'], row)\n",
    "\t\telse:\n",
    "\t\t\treturn node['left']\n",
    "\telse:\n",
    "\t\tif isinstance(node['right'], dict):\n",
    "\t\t\treturn predict(node['right'], row)\n",
    "\t\telse:\n",
    "\t\t\treturn node['right']\n",
    " \n",
    "# Create a random subsample from the dataset with replacement\n",
    "def subsample(dataset, ratio):\n",
    "\tsample = list()\n",
    "\tn_sample = round(len(dataset) * ratio)\n",
    "\twhile len(sample) < n_sample:\n",
    "\t\tindex = randrange(len(dataset))\n",
    "\t\tsample.append(dataset[index])\n",
    "\treturn sample\n",
    "\n",
    "\n",
    "# Make a prediction with a list of bagged trees\n",
    "#responsible for making a prediction with each decision tree and \n",
    "#combining the predictions into a single return value. \n",
    "#This is achieved by selecting the most common prediction \n",
    "#from the list of predictions made by the bagged trees.\n",
    "def bagging_predict(trees, row):\n",
    "\tpredictions = [predict(tree, row) for tree in trees]\n",
    "\treturn max(set(predictions), key=predictions.count)\n",
    " \n",
    "# Random Forest Algorithm\n",
    "#esponsible for creating the samples of the training dataset, training a decision tree on each,\n",
    "#then making predictions on the test dataset using the list of bagged trees.\n",
    "def random_forest(train, test, max_depth, min_size, sample_size, n_trees, n_features):\n",
    "\ttrees = list()\n",
    "\tfor i in range(n_trees):\n",
    "\t\tsample = subsample(train, sample_size)\n",
    "\t\ttree = build_tree(sample, max_depth, min_size, n_features)\n",
    "\t\ttrees.append(tree)\n",
    "\tpredictions = [bagging_predict(trees, row) for row in test]\n",
    "\treturn(predictions)\n",
    " \n",
    "# Test the random forest algorithm\n",
    "seed(1)\n",
    "# load and prepare data\n",
    "filename = 'crop_production.csv'\n",
    "dataset = load_csv(filename)\n",
    "# convert string attributes to integers\n",
    "# for i in range(0, len(dataset[0])-1):\n",
    "# \tstr_column_to_float(dataset, i)\n",
    "# convert class column to integers\n",
    "# str_column_to_int(dataset, len(dataset[0])-1)\n",
    "# evaluate algorithm\n",
    "n_folds = 5\n",
    "max_depth = 10\n",
    "min_size = 1\n",
    "sample_size = 1.0\n",
    "n_features = int(sqrt(len(dataset[0])-1))\n",
    "for n_trees in [1, 5, 10]:\n",
    "\tscores = evaluate_algorithm(dataset, random_forest, n_folds, max_depth, min_size, sample_size, n_trees, n_features)\n",
    "\tprint('Trees: %d' % n_trees)\n",
    "\tprint('Scores: %s' % scores)\n",
    "\tprint('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
